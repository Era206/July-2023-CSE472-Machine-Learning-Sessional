{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3506349,"sourceType":"datasetVersion","datasetId":2110338}],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport glob\nfrom sklearn.metrics import confusion_matrix\nimport IPython.display as ipd  # To play sound in the notebook\nimport os\nimport sys\nimport warnings\n# ignore warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:01:17.946888Z","iopub.execute_input":"2024-07-30T21:01:17.947316Z","iopub.status.idle":"2024-07-30T21:01:29.420274Z","shell.execute_reply.started":"2024-07-30T21:01:17.947275Z","shell.execute_reply":"2024-07-30T21:01:29.419489Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\n\nSUB = \"/kaggle/input/subescobangla-speech-emotion-dataset/SUBESCO\"\nfname = os.listdir(SUB)\nfname.sort()\n\nemotion_mapping = {\n    'SAD': 'sad',\n    'NEUTRAL': 'neutral',\n    'ANGRY': 'angry',\n    'DISGUST': 'disgust',\n    'FEAR': 'fear',\n    'SURPRISE': 'surprise',\n    'HAPPY': 'happy'\n}\n\nemotion = []\ngender = []\npath = []\n\nfor f in fname:\n    parts = f.replace('.wav', '').split('_')\n    # Emotion is second to last part of the split filename\n    file_emotion = parts[-2]\n    file_emotion = emotion_mapping.get(file_emotion, 'unknown')  # Emotion keys are already uppercase\n\n    # Gender is indicated by the first character, 'M' for Male, 'F' for Female\n    file_gender = 'male' if parts[0] == 'M' else 'female'\n\n    emotion.append(file_emotion)\n    gender.append(file_gender)\n    path.append(os.path.join(SUB, f))\n\n# Create a DataFrame\nSUBESCO_df = pd.DataFrame({\n    'labels': [f\"{g}_{e}\" for g, e in zip(gender, emotion)],\n    'source': 'SUBESCO',\n    'path': path\n})\n\n# Save the DataFrame to a CSV file\ncsv_file_path = '/kaggle/working/SUBESCO_data.csv'\nSUBESCO_df.to_csv(csv_file_path, index=False)\n\n# If you want to print out the first few rows of the DataFrame\nprint(SUBESCO_df.head())\n\n# To print the label counts\nprint(SUBESCO_df['labels'].value_counts())\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:02:06.560242Z","iopub.execute_input":"2024-07-30T21:02:06.560623Z","iopub.status.idle":"2024-07-30T21:02:06.653631Z","shell.execute_reply.started":"2024-07-30T21:02:06.560594Z","shell.execute_reply":"2024-07-30T21:02:06.652780Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"         labels   source                                               path\n0  female_angry  SUBESCO  /kaggle/input/subescobangla-speech-emotion-dat...\n1  female_angry  SUBESCO  /kaggle/input/subescobangla-speech-emotion-dat...\n2  female_angry  SUBESCO  /kaggle/input/subescobangla-speech-emotion-dat...\n3  female_angry  SUBESCO  /kaggle/input/subescobangla-speech-emotion-dat...\n4  female_angry  SUBESCO  /kaggle/input/subescobangla-speech-emotion-dat...\nlabels\nfemale_angry       500\nfemale_disgust     500\nfemale_fear        500\nfemale_happy       500\nfemale_neutral     500\nfemale_sad         500\nfemale_surprise    500\nmale_angry         500\nmale_disgust       500\nmale_fear          500\nmale_happy         500\nmale_neutral       500\nmale_sad           500\nmale_surprise      500\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"print(SUBESCO_df)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:02:10.588975Z","iopub.execute_input":"2024-07-30T21:02:10.589773Z","iopub.status.idle":"2024-07-30T21:02:10.598525Z","shell.execute_reply.started":"2024-07-30T21:02:10.589727Z","shell.execute_reply":"2024-07-30T21:02:10.597412Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"             labels   source  \\\n0      female_angry  SUBESCO   \n1      female_angry  SUBESCO   \n2      female_angry  SUBESCO   \n3      female_angry  SUBESCO   \n4      female_angry  SUBESCO   \n...             ...      ...   \n6995  male_surprise  SUBESCO   \n6996  male_surprise  SUBESCO   \n6997  male_surprise  SUBESCO   \n6998  male_surprise  SUBESCO   \n6999  male_surprise  SUBESCO   \n\n                                                   path  \n0     /kaggle/input/subescobangla-speech-emotion-dat...  \n1     /kaggle/input/subescobangla-speech-emotion-dat...  \n2     /kaggle/input/subescobangla-speech-emotion-dat...  \n3     /kaggle/input/subescobangla-speech-emotion-dat...  \n4     /kaggle/input/subescobangla-speech-emotion-dat...  \n...                                                 ...  \n6995  /kaggle/input/subescobangla-speech-emotion-dat...  \n6996  /kaggle/input/subescobangla-speech-emotion-dat...  \n6997  /kaggle/input/subescobangla-speech-emotion-dat...  \n6998  /kaggle/input/subescobangla-speech-emotion-dat...  \n6999  /kaggle/input/subescobangla-speech-emotion-dat...  \n\n[7000 rows x 3 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install resampy","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:02:15.496964Z","iopub.execute_input":"2024-07-30T21:02:15.497340Z","iopub.status.idle":"2024-07-30T21:02:28.809068Z","shell.execute_reply.started":"2024-07-30T21:02:15.497305Z","shell.execute_reply":"2024-07-30T21:02:28.807833Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting resampy\n  Obtaining dependency information for resampy from https://files.pythonhosted.org/packages/4d/b9/3b00ac340a1aab3389ebcc52c779914a44aadf7b0cb7a3bf053195735607/resampy-0.4.3-py3-none-any.whl.metadata\n  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from resampy) (1.24.3)\nRequirement already satisfied: numba>=0.53 in /opt/conda/lib/python3.10/site-packages (from resampy) (0.57.1)\nRequirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.53->resampy) (0.40.1)\nDownloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: resampy\nSuccessfully installed resampy-0.4.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Importing required libraries\n# Keras\nimport keras\nfrom keras import regularizers\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom tensorflow.keras.utils import to_categorical\n\nfrom keras.callbacks import ModelCheckpoint\n\n# sklearn\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Other\n\nimport librosa\nimport librosa.display\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport seaborn as sns\nimport glob\nimport os\nimport pickle\nimport IPython.display as ipd  # To play sound in the notebook","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:02:40.584658Z","iopub.execute_input":"2024-07-30T21:02:40.584991Z","iopub.status.idle":"2024-07-30T21:02:40.594093Z","shell.execute_reply.started":"2024-07-30T21:02:40.584967Z","shell.execute_reply":"2024-07-30T21:02:40.592984Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"ref = pd.read_csv(\"/kaggle/working/SUBESCO_data.csv\")\nref.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:02:56.588801Z","iopub.execute_input":"2024-07-30T21:02:56.589307Z","iopub.status.idle":"2024-07-30T21:02:56.618635Z","shell.execute_reply.started":"2024-07-30T21:02:56.589277Z","shell.execute_reply":"2024-07-30T21:02:56.617779Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"         labels   source                                               path\n0  female_angry  SUBESCO  /kaggle/input/subescobangla-speech-emotion-dat...\n1  female_angry  SUBESCO  /kaggle/input/subescobangla-speech-emotion-dat...\n2  female_angry  SUBESCO  /kaggle/input/subescobangla-speech-emotion-dat...\n3  female_angry  SUBESCO  /kaggle/input/subescobangla-speech-emotion-dat...\n4  female_angry  SUBESCO  /kaggle/input/subescobangla-speech-emotion-dat...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>source</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>female_angry</td>\n      <td>SUBESCO</td>\n      <td>/kaggle/input/subescobangla-speech-emotion-dat...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>female_angry</td>\n      <td>SUBESCO</td>\n      <td>/kaggle/input/subescobangla-speech-emotion-dat...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>female_angry</td>\n      <td>SUBESCO</td>\n      <td>/kaggle/input/subescobangla-speech-emotion-dat...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>female_angry</td>\n      <td>SUBESCO</td>\n      <td>/kaggle/input/subescobangla-speech-emotion-dat...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>female_angry</td>\n      <td>SUBESCO</td>\n      <td>/kaggle/input/subescobangla-speech-emotion-dat...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom torchvision import transforms\n\n\n# Define the transformation for ResNet18\ntransform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Directory to save spectrogram images\nspectrogram_dir = '/kaggle/working/spectogram'\nos.makedirs(spectrogram_dir, exist_ok=True)\n\n# Function to convert audio to spectrogram image and save\ndef audio_to_spectrogram(audio_path, label, save_dir, transform):\n    y, sr = librosa.load(audio_path)\n    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n    log_S = librosa.power_to_db(S, ref=np.max)\n\n    plt.figure(figsize=(3.5, 3.5))\n    librosa.display.specshow(log_S, sr=sr, x_axis='time', y_axis='mel')\n    plt.axis('off')\n    # Use a temporary file path for the spectrogram\n    temp_spec_path = 'temp_spectrogram.png'\n    plt.savefig(temp_spec_path, bbox_inches='tight', pad_inches=0)\n    plt.close()\n\n    # Open the image and apply any necessary transformations\n    image = Image.open(temp_spec_path).convert('RGB')\n\n    # Save the spectrogram image with a label prefix\n    image_save_path = os.path.join(save_dir, f\"{label}_{os.path.basename(audio_path)}.png\")\n    image.save(image_save_path)  # Save the PIL image before it becomes a tensor\n\n    # Now apply the transform and continue with your process\n    if transform:\n        image = transform(image)  # image is now a tensor\ncount=0\n# Iterate over the DataFrame and process each audio file\nfor _, row in ref.iterrows():\n    audio_path = row['path']\n    label = row['labels']\n    audio_to_spectrogram(audio_path, label, spectrogram_dir, transform)\n    count+=1\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:02:59.807308Z","iopub.execute_input":"2024-07-30T21:02:59.808163Z","iopub.status.idle":"2024-07-30T21:25:22.028168Z","shell.execute_reply.started":"2024-07-30T21:02:59.808129Z","shell.execute_reply":"2024-07-30T21:25:22.027323Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torchvision.io import read_image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nclass MelSpectrogramDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.files = [f for f in os.listdir(root_dir) if f.endswith('.png')]\n        \n        # Extract labels and ensure they are unique\n        self.labels = [self._extract_label(f) for f in self.files]\n        self.classes = sorted(set(self.labels))\n        \n        # Create a mapping from label strings to integers\n        self.label_to_index = {label: idx for idx, label in enumerate(self.classes)}\n\n    def _extract_label(self, filename):\n        # Customize this method based on how your labels are encoded in the filenames\n        parts = filename.split('_')\n        label = '_'.join(parts[:2])  # Assuming the label is in the first two parts\n        return label\n    \n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir, self.files[idx])\n        image = read_image(img_name)\n        label_str = self._extract_label(self.files[idx])\n        label = self.label_to_index[label_str]  # Convert label to index\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# Define transformations for the input data\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ConvertImageDtype(torch.float),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Set the directory containing your Mel spectrograms\nmel_spectrogram_dir = '/kaggle/working/spectogram'\n# Create the dataset and dataloader\nmel_dataset = MelSpectrogramDataset(root_dir=mel_spectrogram_dir, transform=transform)\nmel_loader = DataLoader(mel_dataset, batch_size=32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:26:11.804779Z","iopub.execute_input":"2024-07-30T21:26:11.805844Z","iopub.status.idle":"2024-07-30T21:26:11.832754Z","shell.execute_reply.started":"2024-07-30T21:26:11.805808Z","shell.execute_reply":"2024-07-30T21:26:11.832028Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import random_split\n\n# Define the proportion of the dataset to use for training\ntrain_proportion = 0.8\n\n# Calculate the number of samples for train and test sets\nnum_train = int(len(mel_dataset) * train_proportion)\nnum_test = len(mel_dataset) - num_train\n\n# Split the dataset into train and test sets\ntrain_dataset, test_dataset = random_split(mel_dataset, [num_train, num_test])\n\n# Create DataLoaders for train and test sets\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:26:16.369019Z","iopub.execute_input":"2024-07-30T21:26:16.369421Z","iopub.status.idle":"2024-07-30T21:26:16.378532Z","shell.execute_reply.started":"2024-07-30T21:26:16.369395Z","shell.execute_reply":"2024-07-30T21:26:16.377563Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"pip install torch","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:31:08.282391Z","iopub.execute_input":"2024-02-26T08:31:08.282777Z","iopub.status.idle":"2024-02-26T08:31:45.528918Z","shell.execute_reply.started":"2024-02-26T08:31:08.282719Z","shell.execute_reply":"2024-02-26T08:31:45.527379Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0+cpu)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\n# Initialize the model\n\noriginal_dataset = mel_dataset\n\nmodel = models.resnet18(pretrained=True)\nnum_ftrs = model.fc.in_features\n\n# Use the classes from the original ImageFolder dataset\nmodel.fc = nn.Linear(num_ftrs, len(original_dataset.classes))\n\n# Move the model to the GPU if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Define the loss function and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=3e-4)\n\n# Training loop\nnum_epochs = 10  # Set the number of epochs\nfor epoch in range(num_epochs):\n    model.train()  # Set the model to training mode\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = loss_fn(outputs, labels)\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n    # Validation loop\n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Accuracy: {100 * correct / total}%')\n\nprint('Training complete')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T21:33:06.152147Z","iopub.execute_input":"2024-07-30T21:33:06.152529Z","iopub.status.idle":"2024-07-30T21:36:52.895853Z","shell.execute_reply.started":"2024-07-30T21:33:06.152499Z","shell.execute_reply":"2024-07-30T21:36:52.894866Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Epoch [1/10], Validation Accuracy: 70.42857142857143%\nEpoch [2/10], Validation Accuracy: 75.42857142857143%\nEpoch [3/10], Validation Accuracy: 81.64285714285714%\nEpoch [4/10], Validation Accuracy: 83.71428571428571%\nEpoch [5/10], Validation Accuracy: 84.14285714285714%\nEpoch [6/10], Validation Accuracy: 81.42857142857143%\nEpoch [7/10], Validation Accuracy: 81.28571428571429%\nEpoch [8/10], Validation Accuracy: 85.35714285714286%\nEpoch [9/10], Validation Accuracy: 84.85714285714286%\nEpoch [10/10], Validation Accuracy: 89.42857142857143%\nTraining complete\n","output_type":"stream"}]}]}